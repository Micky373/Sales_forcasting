{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'filehandler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Micky\\Desktop\\10Acad\\Sales_forcasting\\notebooks\\data_preprocessing_and_modeing.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Micky/Desktop/10Acad/Sales_forcasting/notebooks/data_preprocessing_and_modeing.ipynb#ch0000001?line=6'>7</a>\u001b[0m DC \u001b[39m=\u001b[39m DataCleaner()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Micky/Desktop/10Acad/Sales_forcasting/notebooks/data_preprocessing_and_modeing.ipynb#ch0000001?line=7'>8</a>\u001b[0m DV \u001b[39m=\u001b[39m Data_Viz()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Micky/Desktop/10Acad/Sales_forcasting/notebooks/data_preprocessing_and_modeing.ipynb#ch0000001?line=8'>9</a>\u001b[0m DT \u001b[39m=\u001b[39m DataTransformer()\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'filehandler'"
     ]
    }
   ],
   "source": [
    "# Importing some scripts\n",
    "\n",
    "from scripts.data_vizualization import Data_Viz \n",
    "from scripts.data_cleaning import DataCleaner\n",
    "from scripts.data_transformation import DataTransformer\n",
    "\n",
    "DC = DataCleaner()\n",
    "DV = Data_Viz()\n",
    "DT = DataTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train data and check for outliers\n",
    "\n",
    "train = pd.read_csv('../data/train_data_clean.csv',index_col='Date')\n",
    "DV.summ_columns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column\n",
    "\n",
    "train.loc[train['DayOfMonth'] >= 20,'DayStatus'] = 'End of Month' \n",
    "train.loc[(train['DayOfMonth'] < 20) & (train['DayOfMonth'] > 10),'DayStatus'] = 'Mid of Month' \n",
    "train.loc[train['DayOfMonth'] < 10,'DayStatus'] = 'Begining of Month' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out if the column worked as planned\n",
    "\n",
    "train['DayStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column for store having competator or not\n",
    "\n",
    "train.loc[train['CompetitionOpenSinceMonth'] != 'Not Available','HasCompetator'] = 1 \n",
    "train.loc[train['CompetitionOpenSinceMonth'] == 'Not Available','HasCompetator'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the catagorical and numerical columns and list out all the numerical columns\n",
    "\n",
    "categorical_col, numerical_col = DT.sep_cat_num(train)\n",
    "numerical_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the most correlated columns and see the heatmap\n",
    "\n",
    "plt.figure(figsize=(4, 8))\n",
    "sns.heatmap(numerical_col.corr().loc['Sales',:].to_frame(), annot=True)\n",
    "plt.show()\n",
    "plt.savefig('../charts/sales_vs_all_corr.jpg')\n",
    "\n",
    "log_artifacts(\"../charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above correlation graph we can see that Weekday, Promo, Open, Customer and DaysOfWeek column has greater impact on sales. So we can take only these columns for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling the data by the above infered correlation and taking into consideration the effect of store type and state holidays\n",
    "\n",
    "useful_columns = ['Sales','DayOfWeek','Customers','Open','StoreType','StateHoliday','Assortment']\n",
    "sampled_df = train[useful_columns]\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find out on which column is sales found\n",
    "\n",
    "train.columns.tolist()[2] # It is the 3rd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our pipe line\n",
    "\n",
    "pipe = Pipeline(steps = [\n",
    "                        (\"labeling\", FunctionTransformer(DT.cat_labeler, kw_args={\"cat_cols\": categorical_col.columns.to_list()})),\n",
    "                        (\"scaling\", FunctionTransformer(DT.scaler)), \n",
    "                        (\"target\", FunctionTransformer(DT.target_feature, kw_args={\"t\":2})),\n",
    "                        (\"split\", FunctionTransformer(DT.set_splitter, kw_args={\"test\": 0.1, \"val\":0.2, \"rand_state\":15}))\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity and effectiveness let take some samples 300K from the 1M \n",
    "\n",
    "sampled_train = train.sample(n=300000)\n",
    "sets = pipe.fit_transform(sampled_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data with random forest regression\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators = 200, random_state = 15)\n",
    "regressor.fit(sets[0], sets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loss function is at its core, measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function. Since we have normalized data with no outliers I prefer to use MSE as it gives more emphasis to errors that will make the prediction more reliable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of our model\n",
    "\n",
    "score = regressor.score(sets[2], sets[3])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a feature list and finding out the feature importance\n",
    "\n",
    "features = train.columns.to_list()\n",
    "features.remove('Sales')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(features, regressor.feature_importances_)\n",
    "plt.savefig('../charts/rf_feature_imp.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the feature importance of customer\n",
    "\n",
    "print(f'The maximum importance value is customer with importance value of: {max(regressor.feature_importances_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the current date and time for our serialization naming\n",
    "\n",
    "now = datetime.now()\n",
    "formated_date = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "formated_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a file name using the formated time\n",
    "\n",
    "file_path = \"../models/\"+str(formated_date)+'.pkl'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing or dumping using pkl loader\n",
    "\n",
    "pickle.dump(regressor,open(file_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read from our pickle serialized model and predict by using our validation data set\n",
    "\n",
    "model = pickle.load(open(file_path,'rb')) # Opening the pkl file and passing rb( read binary) to read it\n",
    "score = model.score(sets[4],sets[5])\n",
    "print(f'The accuracy of the model saved is: {score}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6f8b5314fcad74b5cea59779b57d0410b1657d926ed740031cc86c5ef67808c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
