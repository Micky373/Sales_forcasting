{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries and removing the warning messages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib import ticker\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the scripts\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from scripts.data_vizualization import Data_Viz \n",
    "from scripts.data_cleaning import DataCleaner\n",
    "from scripts.data_transformation import DataTransformer\n",
    "from scripts.stationary_check import check_stationary_mv, check_stationary_adf,corrPlots\n",
    "from scripts.deep_learning import windowed_dataset,model_forecast\n",
    "\n",
    "DC = DataCleaner()\n",
    "DV = Data_Viz()\n",
    "DT = DataTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and reading the data\n",
    "\n",
    "train = pd.read_csv('../data/train_data_clean.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the index and changing it from object to date format and sorting the data by date\n",
    "\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "train = train.sort_values(by='Date')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is a missing value\n",
    "\n",
    "DV.summ_columns(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the above we can see that the train dataset has no missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sales per day dataframe\n",
    "\n",
    "sales_per_day = train[['Date','Sales']].groupby(['Date']).agg({'Sales':'mean'})\n",
    "\n",
    "sales_per_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the sales per day graph\n",
    "\n",
    "DV.plot_line(sales_per_day, \"Date\", \"Sales\", [15, 8], \"Sales per day\", \"sales_per_day.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the above graph we can deduce that the data is more or less stationary with constant mean and std. But in the next section lets try to check its stationarity by using ADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking stationarity both by using ADF and mean value\n",
    "check_stationary_mv(sales_per_day, \"sales per days\")\n",
    "print('\\n************')\n",
    "check_stationary_adf(sales_per_day,\"Sales\", \"sales per days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above metrixes we can see that the p-value is <0.05 and the mean and std are constant so we can say that the data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting autocorrelation graph for sales per day\n",
    "acf_days = acf(sales_per_day.Sales.values, fft=True, nlags=941)\n",
    "acfNp = np.array(acf_days)\n",
    "pacf_days = pacf(sales_per_day.Sales.values, nlags=200)\n",
    "pacfNp = np.array(pacf_days)\n",
    "\n",
    "corrPlots(acfNp, 'Simple')\n",
    "corrPlots(pacfNp, \"Paritial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the above graph also we can see that the autocorrelation is having a constant pattern so we can say that the data is stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data from -1 to 1 for the model fitting\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(sales_per_day.Sales.values.reshape([-1, 1]))\n",
    "SalesScaled = scaler.transform(sales_per_day.Sales.values.reshape(-1, 1))\n",
    "sales_per_day['SalesScaled'] = SalesScaled\n",
    "sales_per_day.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the min max value for the scaled sales data\n",
    "\n",
    "sales_per_day.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the above description we can see that the data is successfuly scaled between -1 and 1 but in our case 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation set separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(sales_per_day.SalesScaled)\n",
    "WINDOW_SIZE = 48\n",
    "BATCH_SIZE= SIZE-WINDOW_SIZE*2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DateTrain = sales_per_day.index.values[0:BATCH_SIZE]\n",
    "DateValid = sales_per_day.index.values[BATCH_SIZE:]\n",
    "XTrain = sales_per_day.SalesScaled.values[0:BATCH_SIZE].astype('float32')\n",
    "XValid = sales_per_day.SalesScaled.values[BATCH_SIZE:].astype('float32')\n",
    "\n",
    "# Obtain shapes for vectors of size (,1) for dates series\n",
    "\n",
    "DateTrain = np.reshape(DateTrain, (-1, 1))\n",
    "DateValid = np.reshape(DateValid, (-1, 1))\n",
    "\n",
    "print(\"Shape of the training set date series: \", DateTrain.shape)\n",
    "print(\"Shape of the validation set date series: \", DateValid.shape)\n",
    "print()\n",
    "print(\"Shape of the training set logarithm of sales series: \", XTrain.shape)\n",
    "print(\"Shape of the validation set logarithm of sales series in a stateless LSTM: \", XValid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "# add extra dimension\n",
    "series = tf.expand_dims(XTrain, axis=-1)\n",
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor from each individual element\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a window_size + 1 chunk from the slices\n",
    "dataset = dataset.window(WINDOW_SIZE + 1, shift=1, drop_remainder=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(WINDOW_SIZE + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE): \n",
    "  series = tf.expand_dims(series, axis=-1)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True) \n",
    "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "  dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
    "  dataset = dataset.batch(batch_size).prefetch(1)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train and validation dataset\n",
    "\n",
    "DatasetTrain = windowed_dataset(XTrain)\n",
    "DatasetVal = windowed_dataset(XValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the deep learning model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(8, input_shape=[None, 1], return_sequences=True))\n",
    "model.add(LSTM(4, input_shape=[None, 1]))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"huber_loss\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the model history and life cycle since its output is very long I removed the output and display the snippet in the next cell\n",
    "\n",
    "history = model.fit(DatasetTrain, epochs=EPOCHS, validation_data=DatasetVal, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The snippet of the above cell output\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "Image('../charts/Capture.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss plot\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.savefig('../charts/deep_learning_model_loss.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the accuracy of the model by using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forecast = model_forecast(model, sales_per_day.SalesScaled.values[:, np.newaxis], WINDOW_SIZE, SIZE)\n",
    "Results = Forecast[BATCH_SIZE-WINDOW_SIZE:-1]\n",
    "Results1 = scaler.inverse_transform(Results.reshape(-1,1))\n",
    "XValid1 = scaler.inverse_transform(XValid.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 8))\n",
    "plt.title(\"LSTM Model Forecast Compared to Validation Data\")\n",
    "plt.plot(DateValid.astype('datetime64'), Results1, label='Forecast series')\n",
    "plt.plot(DateValid.astype('datetime64'), np.reshape(XValid1, (2*WINDOW_SIZE, 1)), label='Validation series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Thousands of Units')\n",
    "plt.xticks(DateValid.astype('datetime64')[:,-1], rotation = 90) \n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "MAE = tf.keras.metrics.mean_absolute_error(XValid1[:,-1], Results[:,-1]).numpy()\n",
    "RMSE = np.sqrt(tf.keras.metrics.mean_squared_error(XValid1[:,-1], Results[:,-1]).numpy())\n",
    "\n",
    "textstr = \"MAE = \" + \"{:.3f}\".format(MAE) + \"  RMSE = \" + \"{:.3f}\".format(RMSE)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "plt.annotate(textstr, xy=(0.87, 0.05), xycoords='axes fraction')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "print(textstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(sales_per_day)\n",
    "sales_per_day['DataScaled'] = scaled_array\n",
    "BATCH_SIZE = len(sales_per_day) - (WINDOW_SIZE * 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6f8b5314fcad74b5cea59779b57d0410b1657d926ed740031cc86c5ef67808c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
